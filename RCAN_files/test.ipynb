{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"abeinasdfasbijndfbidnbdinae\".find(\"bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]],\n",
       "\n",
       "       [[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]],\n",
       "\n",
       "       [[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[1,2,3],[4,5,6],[7,8,9]]])\n",
    "b = np.vstack([a,a,a])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32344064513842263"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = 0\n",
    "num = 0\n",
    "for file in os.listdir(\"train_dataset/train_lr/\"):\n",
    "    img = Image.open(\"train_dataset/train_lr/\"+file)\n",
    "    img = np.array(img,dtype=np.float32)\n",
    "    if 256 <= np.max(img) <= 65535:\n",
    "        img //= 256.\n",
    "    if 1 < np.max(img) <= 255:\n",
    "        img /= 255.\n",
    "    tot += np.mean(img)\n",
    "    num += 1\n",
    "tot/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35000471224387486"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot = 0\n",
    "num = 0\n",
    "for file in os.listdir(\"train_dataset/train_hr/\"):\n",
    "    img = Image.open(\"train_dataset/train_hr/\"+file)\n",
    "    img = np.array(img,dtype=np.float32)\n",
    "    if 256 <= np.max(img) <= 65535:\n",
    "        img //= 256.\n",
    "    if 1 < np.max(img) <= 255:\n",
    "        img /= 255.\n",
    "    tot += np.mean(img)\n",
    "    num += 1\n",
    "tot/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanShift(nn.Conv2d):\n",
    "    def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):\n",
    "        super(MeanShift, self).__init__(3, 3, kernel_size=1)\n",
    "        std = torch.Tensor(rgb_std)\n",
    "        self.weight.data = torch.eye(3).view(3, 3, 1, 1)\n",
    "        self.weight.data.div_(std.view(3, 1, 1, 1))\n",
    "        self.bias.data = sign * rgb_range * torch.Tensor(rgb_mean)\n",
    "        self.bias.data.div_(std)\n",
    "        self.requires_grad = False\n",
    "\n",
    "class MeanShiftGrey(nn.Conv2d):\n",
    "    def __init__(self, rgb_range, rgb_mean, rgb_std, sign=-1):\n",
    "        super(MeanShiftGrey, self).__init__(1, 1, kernel_size=1)\n",
    "        std = torch.tensor([rgb_std])\n",
    "        self.weight.data = torch.eye(1).view(1, 1, 1, 1)\n",
    "        self.weight.data.div_(std.view(1, 1, 1, 1))\n",
    "        self.bias.data = sign * rgb_range * torch.tensor([rgb_mean])\n",
    "        self.bias.data.div_(std)\n",
    "        self.requires_grad = False\n",
    "        print(self.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Some:\n",
    "    def __init__(self):\n",
    "        rgb_mean = (0.4488, 0.4371, 0.4040)\n",
    "        rgb_std = (1.0, 1.0, 1.0)\n",
    "        self.sub_mean1 = MeanShift(255, rgb_mean, rgb_std)\n",
    "        self.add_mean1 = MeanShift(255, rgb_mean, rgb_std, 1)\n",
    "        self.sub_mean2 = MeanShiftGrey(255, (0.4488), (1.0))\n",
    "        self.add_mean2 = MeanShiftGrey(255, (0.4488), (1.0), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sub_mean1(x)\n",
    "        y = self.add_mean1(x)\n",
    "        return x,y\n",
    "    \n",
    "    def forward2(self, x):\n",
    "        x = self.sub_mean2(x)\n",
    "        y = self.add_mean2(x)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.randint(0,255,[2,3,5,5],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "something = Some()\n",
    "x = torch.tensor(arr,dtype=torch.float32)\n",
    "y,z = something.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "other = Some()\n",
    "c = torch.tensor(arr[:,0].reshape(2,1,5,5),dtype=torch.float32)\n",
    "a,b = other.forward2(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 27.,  37.,  23., 251., 135.],\n",
       "          [148., 249., 227.,  90., 226.],\n",
       "          [ 51., 122., 195., 125., 173.],\n",
       "          [104.,   1.,  44., 212., 137.],\n",
       "          [205., 141., 121., 230., 208.]]],\n",
       "\n",
       "\n",
       "        [[[164., 110., 207., 236., 169.],\n",
       "          [  7.,  14.,  36.,  20.,  39.],\n",
       "          [157.,  15., 195.,  77.,  15.],\n",
       "          [ 28.,  29.,  37.,  54.,  13.],\n",
       "          [188.,  82., 112., 114., 143.]]]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 27.,  37.,  23., 251., 135.],\n",
       "          [148., 249., 227.,  90., 226.],\n",
       "          [ 51., 122., 195., 125., 173.],\n",
       "          [104.,   1.,  44., 212., 137.],\n",
       "          [205., 141., 121., 230., 208.]],\n",
       "\n",
       "         [[212.,  77.,  45.,   9., 230.],\n",
       "          [ 87., 176.,  42., 246.,   2.],\n",
       "          [197., 235., 162.,  52.,  13.],\n",
       "          [ 94., 242.,  47., 127., 172.],\n",
       "          [211.,  77., 173., 200., 239.]],\n",
       "\n",
       "         [[249., 202., 243.,  15., 120.],\n",
       "          [224., 197.,  82., 232., 231.],\n",
       "          [138., 228., 243., 235., 254.],\n",
       "          [250., 226.,  51.,  17., 177.],\n",
       "          [115.,  99., 178., 231., 234.]]],\n",
       "\n",
       "\n",
       "        [[[164., 110., 207., 236., 169.],\n",
       "          [  7.,  14.,  36.,  20.,  39.],\n",
       "          [157.,  15., 195.,  77.,  15.],\n",
       "          [ 28.,  29.,  37.,  54.,  13.],\n",
       "          [188.,  82., 112., 114., 143.]],\n",
       "\n",
       "         [[150., 222., 111., 155.,  43.],\n",
       "          [116.,   4., 181.,  61.,  11.],\n",
       "          [ 18.,  20., 194., 207., 145.],\n",
       "          [ 34., 146., 193.,  95.,  33.],\n",
       "          [229., 195.,  96., 251.,  63.]],\n",
       "\n",
       "         [[162.,  32.,  90., 111.,  14.],\n",
       "          [228.,  95.,  50., 188., 117.],\n",
       "          [182., 178.,  82.,  80., 171.],\n",
       "          [101., 162., 155., 132.,  38.],\n",
       "          [124., 160., 201., 116.,  55.]]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -87.4440,  -77.4440,  -91.4440,  136.5560,   20.5560],\n",
       "         [  33.5560,  134.5560,  112.5560,  -24.4440,  111.5560],\n",
       "         [ -63.4440,    7.5560,   80.5560,   10.5560,   58.5560],\n",
       "         [ -10.4440, -113.4440,  -70.4440,   97.5560,   22.5560],\n",
       "         [  90.5560,   26.5560,    6.5560,  115.5560,   93.5560]],\n",
       "\n",
       "        [[ 100.5395,  -34.4605,  -66.4605, -102.4605,  118.5395],\n",
       "         [ -24.4605,   64.5395,  -69.4605,  134.5395, -109.4605],\n",
       "         [  85.5395,  123.5395,   50.5395,  -59.4605,  -98.4605],\n",
       "         [ -17.4605,  130.5395,  -64.4605,   15.5395,   60.5395],\n",
       "         [  99.5395,  -34.4605,   61.5395,   88.5395,  127.5395]],\n",
       "\n",
       "        [[ 145.9800,   98.9800,  139.9800,  -88.0200,   16.9800],\n",
       "         [ 120.9800,   93.9800,  -21.0200,  128.9800,  127.9800],\n",
       "         [  34.9800,  124.9800,  139.9800,  131.9800,  150.9800],\n",
       "         [ 146.9800,  122.9800,  -52.0200,  -86.0200,   73.9800],\n",
       "         [  11.9800,   -4.0200,   74.9800,  127.9800,  130.9800]]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -87.4440,  -77.4440,  -91.4440,  136.5560,   20.5560],\n",
       "          [  33.5560,  134.5560,  112.5560,  -24.4440,  111.5560],\n",
       "          [ -63.4440,    7.5560,   80.5560,   10.5560,   58.5560],\n",
       "          [ -10.4440, -113.4440,  -70.4440,   97.5560,   22.5560],\n",
       "          [  90.5560,   26.5560,    6.5560,  115.5560,   93.5560]]],\n",
       "\n",
       "\n",
       "        [[[  49.5560,   -4.4440,   92.5560,  121.5560,   54.5560],\n",
       "          [-107.4440, -100.4440,  -78.4440,  -94.4440,  -75.4440],\n",
       "          [  42.5560,  -99.4440,   80.5560,  -37.4440,  -99.4440],\n",
       "          [ -86.4440,  -85.4440,  -77.4440,  -60.4440, -101.4440],\n",
       "          [  73.5560,  -32.4440,   -2.4440,   -0.4440,   28.5560]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 91., 253., 173., 252., 147.],\n",
       "         [172.,  80., 200.,  39.,  98.],\n",
       "         [215., 162., 182.,  87., 210.],\n",
       "         [113., 193., 176.,  88., 182.],\n",
       "         [  7.,  74.,   4.,  80., 217.]],\n",
       "\n",
       "        [[192.,  19., 146., 238., 150.],\n",
       "         [238.,  53., 233., 114., 211.],\n",
       "         [228., 251.,  28., 232., 140.],\n",
       "         [ 14.,  81., 240.,  48., 143.],\n",
       "         [ 24., 251., 224.,   9., 128.]],\n",
       "\n",
       "        [[116.,  58.,   2., 225., 254.],\n",
       "         [ 60.,  24., 215., 239., 201.],\n",
       "         [  0., 178.,  59., 156., 188.],\n",
       "         [224., 145., 195., 192., 142.],\n",
       "         [196.,  31., 254.,  11.,  63.]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[187., 219.,  49., 140., 109.],\n",
       "         [170., 160.,  16.,  21., 200.],\n",
       "         [176., 247., 238.,  16., 253.],\n",
       "         [140.,  32., 203.,  23., 181.],\n",
       "         [165., 214., 160., 123., 116.]],\n",
       "\n",
       "        [[ 48., 241., 139., 111., 196.],\n",
       "         [ 66.,  65., 254., 246.,  82.],\n",
       "         [142.,   0.,  36., 202., 248.],\n",
       "         [210.,  45., 167.,  78.,  41.],\n",
       "         [ 91., 191., 132.,   7.,  79.]],\n",
       "\n",
       "        [[163., 206.,  92., 102., 165.],\n",
       "         [  9., 164., 176., 197.,  75.],\n",
       "         [  7.,  83.,  76.,  23., 141.],\n",
       "         [216., 192.,  70., 112., 121.],\n",
       "         [ 16.,  47.,  27.,  26.,  52.]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5, 5])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 5])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2.1,3.8]).cuda().device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc as misc\n",
    "from imageio import imsave"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
